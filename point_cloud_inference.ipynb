{"cells":[{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vA4GAvI3bbTg","executionInfo":{"status":"ok","timestamp":1727494339329,"user_tz":-180,"elapsed":1112,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"210ce839-39e0-48f1-8537-757dc4fdab37"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Sep 28 03:32:18 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-K5SH1mWPzL","executionInfo":{"status":"ok","timestamp":1727494373388,"user_tz":-180,"elapsed":34068,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"7b6b9b35-c9a6-4e70-dbd7-36fd82f26f85"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hl9DRIo7A6b","outputId":"2b4a06e6-6a44-4220-d4b5-809925a21c76","executionInfo":{"status":"ok","timestamp":1727494377796,"user_tz":-180,"elapsed":4414,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch + Cuda Version: 2.4.1+cu121\n","Cuda Version Requied: 12.1\n","Cuda Available: True\n","Cuda Device Count: 1\n","Python 3.10.12\n"]}],"source":["import torch\n","print(f\"Torch + Cuda Version: {torch.__version__}\")\n","print(f\"Cuda Version Requied: {torch.version.cuda}\")\n","print(f\"Cuda Available: {torch.cuda.is_available()}\")\n","print(f\"Cuda Device Count: {torch.cuda.device_count()}\")\n","# print(f\"Cuda Device Name: {torch.cuda.get_device_name(0)}\")\n","!python --version"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/MeshAnythingV2\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Zyc9IGHcp_6","executionInfo":{"status":"ok","timestamp":1727494377796,"user_tz":-180,"elapsed":10,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"7044f7f5-91c4-4628-b472-d9348b28f6d8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MeshAnythingV2\n","adjacent_mesh_tokenization.py  \u001b[0m\u001b[01;34mgt_examples\u001b[0m/   mesh_to_pc.py  point_cloud_inference.ipynb\n","app.py                         LICENSE.txt    \u001b[01;34mnpy_files\u001b[0m/     \u001b[01;34m__pycache__\u001b[0m/\n","\u001b[01;34mdemo\u001b[0m/                          main.py        \u001b[01;34moutput\u001b[0m/        README.md\n","\u001b[01;34mexamples\u001b[0m/                      \u001b[01;34mMeshAnything\u001b[0m/  \u001b[01;34mpc_examples\u001b[0m/   requirements.txt\n"]}]},{"cell_type":"code","source":["CURRENCT_WORKING_DIR = %pwd\n","print(f\"Current Working Directory Now: '{CURRENCT_WORKING_DIR}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZAz9cC2eiil","executionInfo":{"status":"ok","timestamp":1727494385217,"user_tz":-180,"elapsed":508,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"2eccee49-602f-43e7-af37-f34878e677cb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory Now: '/content/drive/MyDrive/MeshAnythingV2'\n"]}]},{"cell_type":"code","source":["BASE_DIR = \"/content/drive/MyDrive/MeshAnythingV2/\"\n","NPY_FILES_DIR = BASE_DIR + \"npy_files/\"\n","\n","POINT_CLOUD_JSON = \"/content/drive/MyDrive/Data/sample/data/08cf73ec-313d-436b-8ed9-c2b3034844ba/dsm.json\"\n","# INPUT_POINT_CLOUD = \"/content/drive/MyDrive/MeshAnythingV2/pc_examples/grenade.npy\"\n","NPY_file = NPY_FILES_DIR + POINT_CLOUD_JSON.split(\"/\")[-1].split(\".\")[0] + \".npy\"\n","\n","OUTPUT_DIR = \"/content/drive/MyDrive/MeshAnythingV2/output\""],"metadata":{"id":"vzQeau3idhP9","executionInfo":{"status":"ok","timestamp":1727494454700,"user_tz":-180,"elapsed":395,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!mkdir -p {NPY_FILES_DIR}"],"metadata":{"id":"-jrW2PugoBXE","executionInfo":{"status":"ok","timestamp":1727494468764,"user_tz":-180,"elapsed":377,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!mkdir -p {OUTPUT_DIR}"],"metadata":{"id":"mpfCeLsLdgXC","executionInfo":{"status":"ok","timestamp":1727494476837,"user_tz":-180,"elapsed":423,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/MeshAnythingV2/requirements.txt -q\n","!pip install flash-attn --no-build-isolation -q\n","!pip install open3d -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pV_Ym14lcqC0","executionInfo":{"status":"ok","timestamp":1727495090306,"user_tz":-180,"elapsed":36176,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"f2b0cd5b-b29b-4ba8-d8ca-d5d1d1ffa01c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import open3d as o3d\n","from typing import Any\n","\n","# Step 1: Load the JSON file containing the XYZ coordinates\n","def load_json(json_file) -> np.ndarray:\n","    with open(json_file, 'r') as f:\n","        data: Any = json.load(f)\n","    return np.array(data)  # Convert the list of lists into a NumPy array\n","\n","# Step 2: Compute normals using Open3D\n","def compute_normals(xyz_points) -> np.ndarray:\n","    # Create an Open3D PointCloud object\n","    pcd: Any = o3d.geometry.PointCloud()\n","    pcd.points = o3d.utility.Vector3dVector(xyz_points)\n","\n","    # Estimate the normals\n","    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n","\n","    # Retrieve the normals\n","    normals: np.ndarray[Any, np.dtype[Any]] = np.asarray(pcd.normals)\n","    return normals\n","\n","# Step 3: Save as .npy in the (N, 6) pc_normal format\n","def save_as_npy(xyz_points, normals, npy_file):\n","    # Combine the XYZ coordinates with their corresponding normals\n","    pc_normal_data = np.hstack((xyz_points, normals))\n","\n","    # Save the array to an .npy file\n","    np.save(npy_file, pc_normal_data)\n","    print(f\"Point cloud with normals saved to {npy_file}\")\n","\n","\n","# Example usage\n","xyz_points = load_json(POINT_CLOUD_JSON)\n","normals = compute_normals(xyz_points)\n","save_as_npy(xyz_points, normals, NPY_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"ddHSj-KVxJTz","executionInfo":{"status":"ok","timestamp":1727495938113,"user_tz":-180,"elapsed":3756,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"7f482fba-c045-438d-a94c-386db7aec70b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Point cloud with normals saved to /content/drive/MyDrive/MeshAnythingV2/npy_files/dsm.npy\n"]}]},{"cell_type":"code","source":["# inference for single file\n","!python main.py --input_path {NPY_file} --out_dir {OUTPUT_DIR} --input_type pc_normal"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3BIhe8mcqGB","executionInfo":{"status":"ok","timestamp":1727495972435,"user_tz":-180,"elapsed":29672,"user":{"displayName":"Mazhar","userId":"02389139569705527306"}},"outputId":"d37e1e19-a735-42a9-c05a-34eac7f42e46"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","/content/drive/MyDrive/MeshAnythingV2/MeshAnything/miche/michelangelo/models/modules/checkpoint.py:41: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  def forward(ctx, run_function, length, *args):\n","/content/drive/MyDrive/MeshAnythingV2/MeshAnything/miche/michelangelo/models/modules/checkpoint.py:52: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n","  def backward(ctx, *output_grads):\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","You are using a model of type opt to instantiate a model of type shape_opt. This is not supported for all configurations of models and can yield errors.\n","The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n","You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n","Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ShapeOPT is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n","Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ShapeOPTModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n","Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ShapeOPTDecoder is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n","2024-09-28 03:59:17.438983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-28 03:59:17.456205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-28 03:59:17.461306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-09-28 03:59:18.704439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n","dataset total data samples: 1\n","Generation Start!!!\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/MeshAnythingV2/main.py\", line 133, in <module>\n","    outputs = model(batch_data_label['pc_normal'], sampling=args.sampling)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 822, in forward\n","    return model_forward(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 810, in __call__\n","    return convert_to_fp32(self.model_forward(*args, **kwargs))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MeshAnythingV2/MeshAnything/models/meshanything_v2.py\", line 121, in forward\n","    results = self.transformer.generate(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1527, in generate\n","    result = self._greedy_search(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2411, in _greedy_search\n","    outputs = self(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MeshAnythingV2/MeshAnything/models/shape_opt.py\", line 124, in forward\n","    outputs = self.model.decoder(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/MyDrive/MeshAnythingV2/MeshAnything/models/shape_opt.py\", line 363, in forward\n","    layer_outputs = decoder_layer(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 535, in forward\n","    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 367, in forward\n","    attn_output = self._flash_attention_forward(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 433, in _flash_attention_forward\n","    attn_output = flash_attn_func(\n","  File \"/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\", line 880, in flash_attn_func\n","    return FlashAttnFunc.apply(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 574, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\", line 546, in forward\n","    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_forward(\n","  File \"/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py\", line 52, in _flash_attn_forward\n","    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.fwd(\n","RuntimeError: FlashAttention only supports Ampere GPUs or newer.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"f7vFzj-ZcqI9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}